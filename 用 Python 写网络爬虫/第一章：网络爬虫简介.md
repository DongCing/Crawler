## 第 1 章 网络爬虫简介

### 1.1 网络爬虫何时有用

在理想状态下，网络爬虫并不是必需品，每个网站都应该提供 API，以结构化的格式共享它们的数据。

在现实情况中，虽然一些网站已经提供了这种 API，但是它们通常会限制可以抓取的数据，以及访问这些数据的频率。

### 1.2 网络爬虫是否合法 

当你抓取某个网站的数据时，请记住自己是该网站的访客，应当约束自己的抓取行为，否则他们可能会封禁你的 IP，甚至采取更进一步的法律行动。

这就要求下载请求的速度需要限定在一个合理值之内，并且还需要设定一个专属的用户代理来标识自己的爬虫。

你还应该设法查看网站的服务条款，确保你所获取的数据不是私有或受版权保护的内容。

### 1.3 Python 3

### 1.4 背景调研

爬取一个网站之前，我们首先需要对目标站点的规模和结构进行一定程度的了解。

网站自身的 robots.txt 和 Sitemap 文件都可以为我们提供一定的帮助，此外还有一些能提供更详细信息的外部工具，比如 Google 搜索和 WHOIS。

#### 1.4.1 检查 robots.txt

大多数网站都会定义 robots.txt 文件，这样可以让爬虫了解爬取该网站时存在哪些限制。

robots.txt 协议的更多信息可以参见 http://www.robotstxt.org

在爬取之前，检查 robots.txt 文件这一宝贵资源可以将爬虫被封禁的可能性降至最低，而且还能发现和网站结构相关的线索。

```
# 下面的代码是示例文件 robots.txt 中的内容

# section 1
User-agent: BadCrawler
Disallow: /

# section 2
User-agent: *
Crawl-delay: 5
Disallow: /trap

# section 3
Sitemap: http://example.python-scraping.com/sitemap.xml 
```

* section 1 中，robots.txt 文件禁止用户代理为 BadCrawler 的爬虫爬取该网站，不过这种写法可能无法起到应有的作用，因为恶意爬虫根本不会遵从 robots.txt 的要求。

* section 2 规定，无论使用哪种用户代理，都应该在两次下载请求之间给出5 秒的抓取延迟。还有一个 /trap 链接，用于封禁那些爬取了不允许访问的链接的恶意爬虫。如果你访问了这个链接，服务器就会封禁你的 IP 一分钟！一个真实的网站可能会对你的 IP 封禁更长时间，甚至是永久封禁。

* section 3 定义了一个 Sitemap 文件（即网站地图）

#### 1.4.2  检查网站地图 

网站提供的Sitemap文件（即网站地图）可以帮助爬虫定位网站最新的内容，而无须爬取每一个网页。

网站地图提供了所有网页的链接，但是该文件可能存在缺失、过期或不完整的问题。

#### 1.4.3  估算网站大小 

爬取网站中包含有用数据的部分，而不是爬取网站的每个页面。

#### 1.4.4  识别网站所用技术 

构建网站所使用的技术类型也会对我们如何爬取产生影响。

有一个十分有用的工具可以检查网站构建的技术类型—— detectem 模块，该模块需要 Python 3.5+ 环境以及 Docker。

#### 1.4.5  寻找网站所有者 

为了找到网站的所有者，我们可以使用 WHOIS 协议查询域名的注册者是谁。

Python 中有一个针对该协议的封装库，其文档地址为 https://pypi.python.org/ pypi/python-whois。

pip install python-whois 

```
# 使用该模块对 appspot.com 这个域名进行WHOIS查询

import whois 
print(whois.whois('appspot.com'))   
```

### 1.5  编写第一个网络爬虫 

爬取（crawling）

3 种爬取网站的常见方法：

* 爬取网站地图；

* 使用数据库 ID 遍历每个网页；

* 跟踪网页链接。

#### 1.5.1  抓取与爬取的对比 

网络抓取通常针对特定网站，并在这些站点上获取指定信息。网络抓取用于访问这些特定的页面，如果站点发生变化或者站点中的信息位置发生变化的话，则需要进行修改。

网络爬取通常是以通用的方式构建的，其目标是一系列顶级域名的网站或是整个网络。爬取可以用来收集更具体的信息，不过更常见的情况是爬取网络，从许多不同的站点或页面中获取小而通用的信息，然后跟踪链接到其他页面中。

#### 1.5.2  下载网页 

```python
# 使用 Python 的 urllib 模块下载 URL

import urllib.request 
def download(url):     
    return urllib.request.urlopen(url).read()

# 当传入URL参数时，该函数将会下载网页并返回其 HTML。
# 不过，这个代码片段存在一个问题，即当下载网页时，我们可能会遇到一些无法控制的错误，比如请求的页面可能不存在。
# 此时，urllib 会抛出异常，然后退出脚本。
```

```python
# 更稳建的版本，可以捕获这些异常

import urllib.request  
from urllib.error import URLError, HTTPError, ContentTooShortError 

def download(url):     
    print('Downloading:', url)     
    try:         
        html = urllib.request.urlopen(url).read()     
    except (URLError, HTTPError, ContentTooShortError) as e:         
        print('Download error:', e.reason)         
        html = None     
    return html

# 当出现下载或 URL 错误时，该函数能够捕获到异常，然后返回 None。
```

1．重试下载

下载时遇到的错误经常是临时性的，比如服务器过载时返回的 503 Service Unavailable 错误。

对于此类错误，我们可以在短暂等待后尝试重新下载。

```python
# 支持重试下载功能的新版本代码

import urllib.request  
from urllib.error import URLError, HTTPError, ContentTooShortError 

def download(url, num_retries=2):     
    print('Downloading:', url)     
    try:         
        html = urllib.request.urlopen(url).read()     
    except (URLError, HTTPError, ContentTooShortError) as e:         
        print('Download error:', e.reason)         
        html = None         
        if num_retries > 0:                
            if hasattr(e, 'code') and 500 <= e.code < 600:              
                # recursively retry 5xx HTTP errors              
                return download(url, num_retries - 1)     
    return html

# 该函数还增加了一个参数，用于设定重试下载的次数，其默认值为两次。

# 想要测试该函数，可以尝试下载 http://httpstat.us/500，该网址会始终返回 500 错误码。
```

2．设置用户代理

默认情况下，urllib 使用 Python-urllib/3.x 作为用户代理下载网页内容，其中 3.x 是环境当前所用 Python 的版本号。

为了使下载网站更加可靠，我们需要控制用户代理的设定。

```python
# 设定了一个默认的用户代理‘wswp’（即Web Scraping with Python的首字母缩写）

import urllib.request  
from urllib.error import URLError, HTTPError, ContentTooShortError 

def download(url, user_agent='wswp', num_retries=2):     
    print('Downloading:', url)     
    request = urllib.request.Request(url)     
    request.add_header('User-agent', user_agent)     
    try:         
        html = urllib.request.urlopen(request).read()     
    except (URLError, HTTPError, ContentTooShortError) as e:         
        print('Download error:', e.reason)         
        html = None         
        if num_retries > 0:             
            if hasattr(e, 'code') and 500 <= e.code < 600:
                # recursively retry 5xx HTTP errors              
                return download(url, num_retries - 1)     
    return html

# 再次尝试访问 meetup.com，就能够看到一个合法的 HTML 了
```

#### 1.5.3  网站地图爬虫 

为了解析网站地图，我们将会使用一个简单的正则表达式，从 <loc> 标签中提取出 URL。

```python

import re 
import urllib.request  
from urllib.error import URLError, HTTPError, ContentTooShortError 

def download(url, user_agent='wswp', num_retries=2, charset='utf-8'):     
    print('Downloading:', url)     
    request = urllib.request.Request(url)     
    request.add_header('User-agent', user_agent)     
    try:         
        resp = urllib.request.urlopen(request)         
        cs = resp.headers.get_content_charset()         
        if not cs:             
            cs = charset         
        html = resp.read().decode(cs)     
    except (URLError, HTTPError, ContentTooShortError) as e:         
        print('Download error:', e.reason)         
        html = None         
        if num_retries > 0:             
            if hasattr(e, 'code') and 500 <= e.code < 600:             
                # recursively retry 5xx HTTP errors             
                return download(url, num_retries - 1)     
    return html
    
def crawl_sitemap(url):     
    # download the sitemap file     
    sitemap = download(url)
    
    # extract the sitemap links     
    links = re.findall('<loc>(.*?)</loc>', sitemap)
    
    # download each link     
    for link in links:         
        html = download(link)         
        # scrape html here         
        # ...

```

正如上面代码中的 download 方法所示，我们必须更新字符编码才能利用正则表达式处理网站响应。

Python 的 read 方法返回字节，而正则表达式期望的则是字符串。我们的代码依赖于网站维护者在响应头中包含适当的字符编码。

如果没有返回字符编码头部，我们将会把它设置为默认值 UTF-8。

当然，如果返回头中的编码不正确，或是编码没有设置并且也不是 UTF-8 的话，则会抛出错误。

还有一些更复杂的方式用于猜测编码（参见https://pypi.python.org/pypi/chardet）， 该方法非常容易实现。

