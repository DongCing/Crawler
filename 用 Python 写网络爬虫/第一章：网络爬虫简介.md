## 第 1 章 网络爬虫简介

### 1.1 网络爬虫何时有用

在理想状态下，网络爬虫并不是必需品，每个网站都应该提供 API，以结构化的格式共享它们的数据。

在现实情况中，虽然一些网站已经提供了这种 API，但是它们通常会限制可以抓取的数据，以及访问这些数据的频率。

### 1.2 网络爬虫是否合法 

当你抓取某个网站的数据时，请记住自己是该网站的访客，应当约束自己的抓取行为，否则他们可能会封禁你的 IP，甚至采取更进一步的法律行动。

这就要求下载请求的速度需要限定在一个合理值之内，并且还需要设定一个专属的用户代理来标识自己的爬虫。

你还应该设法查看网站的服务条款，确保你所获取的数据不是私有或受版权保护的内容。

### 1.3 Python 3

### 1.4 背景调研

爬取一个网站之前，我们首先需要对目标站点的规模和结构进行一定程度的了解。

网站自身的 robots.txt 和 Sitemap 文件都可以为我们提供一定的帮助，此外还有一些能提供更详细信息的外部工具，比如 Google 搜索和 WHOIS。

#### 1.4.1 检查 robots.txt

大多数网站都会定义 robots.txt 文件，这样可以让爬虫了解爬取该网站时存在哪些限制。

在爬取之前，检查 robots.txt 文件这一宝贵资源可以将爬虫被封禁的可能性降至最低，而且还能发现和网站结构相关的线索。

```
# 下面的代码是示例文件 robots.txt 中的内容

# section 1
User-agent: BadCrawler
Disallow: /

# section 2
User-agent: *
Crawl-delay: 5
Disallow: /trap

# section 3
Sitemap: http://example.python-scraping.com/sitemap.xml 
```

* section 1 中，robots.txt 文件禁止用户代理为 BadCrawler 的爬虫爬取该网站，不过这种写法可能无法起到应有的作用，因为恶意爬虫根本不会遵从 robots.txt 的要求。

* section 2 规定，无论使用哪种用户代理，都应该在两次下载请求之间给出5 秒的抓取延迟。还有一个 /trap 链接，用于封禁那些爬取了不允许访问的链接的恶意爬虫。如果你访问了这个链接，服务器就会封禁你的 IP 一分钟！一个真实的网站可能会对你的 IP 封禁更长时间，甚至是永久封禁。

* section 3 定义了一个 Sitemap 文件
